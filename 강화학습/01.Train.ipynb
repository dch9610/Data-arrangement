{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow 버전 변경\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 필요한 설정값을 정의한다.\n",
    "\n",
    "# epsilon 값\n",
    "epsilon = 1\n",
    "\n",
    "# epsilon 최소값\n",
    "epsilonMinimumValue = 0.001\n",
    "\n",
    "# 에이전트의 행동 개수 (좌로 움직이기, 가만히 있기, 우로 움직이기)\n",
    "num_actions = 3\n",
    "\n",
    "# 학습 반복횟수\n",
    "num_epochs = 1\n",
    "\n",
    "# 은닉층의 개수\n",
    "hidden_size = 128\n",
    "\n",
    "# Replay Memory 크기 (과거 행위를 기억하기 위한 공간)\n",
    "maxMemory = 500\n",
    "\n",
    "# batch_size \n",
    "batch_size = 5\n",
    "\n",
    "# 환경크기 (게임판 크기)\n",
    "gridSize = 10\n",
    "\n",
    "# 게임 환경의 현재 상태 (10x10)\n",
    "state_size = gridSize * gridSize\n",
    "\n",
    "# 규제 강도\n",
    "discount = 0.9\n",
    "\n",
    "# 학습률\n",
    "learning_range = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작과 끝값을 기준으로 랜덤값을 추출하는 함수를 정의한다.\n",
    "def randf(s, e):\n",
    "    return (float(random.randrange(0, (e - s) * 9999)) / 10000) + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN 모델을 정의한다.\n",
    "def build_DQN(x):\n",
    "    # 1신경망\n",
    "    # 표준화를 위한 계수\n",
    "    a1 = 1.0 / math.sqrt(float(state_size))\n",
    "    W1 = tf.Variable(tf.truncated_normal(shape=[state_size, hidden_size], stddev=a1))\n",
    "    b1 = tf.Variable(tf.truncated_normal(shape=[hidden_size], stddev=0.01))\n",
    "    H1_output = tf.nn.relu(tf.matmul(x,W1)+b1)\n",
    "    \n",
    "    # 2신경망\n",
    "    # 표준화를 위한 계수\n",
    "    a2 = 1.0 / math.sqrt(float(hidden_size))\n",
    "    W2 = tf.Variable(tf.truncated_normal(shape=[hidden_size, hidden_size], stddev=a2))\n",
    "    b2 = tf.Variable(tf.truncated_normal(shape=[hidden_size], stddev=0.01))\n",
    "    H2_output = tf.nn.relu(tf.matmul(H1_output,W2)+b2)\n",
    "    \n",
    "    # 3신경망 (출력층)\n",
    "    W3 = tf.Variable(tf.truncated_normal(shape=[hidden_size, num_actions], stddev=a2))\n",
    "    b3 = tf.Variable(tf.truncated_normal(shape=[num_actions], stddev=0.01))\n",
    "    output_layer= tf.matmul(H2_output,W3) + b3\n",
    "    \n",
    "    return tf.squeeze(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력화면 이미지와 타겟 Q값을 받기위한 플레이스 홀더 (메모리상의 저장공간)\n",
    "x = tf.placeholder(tf.float32, shape=[None, state_size])\n",
    "y = tf.placeholder(tf.float32, shape=[None, num_actions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN 모델을 선언한고 예측 결과를 리턴 받는다.\n",
    "y_pred = build_DQN(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE (손실함수)와 옵티마이저를 정의한다.\n",
    "loss = tf.reduce_sum(tf.square(y - y_pred)) / (2 * batch_size)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_range).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 게임 환경을 구현한다.\n",
    "class CatchEnviroment():\n",
    "    # 상태의 초기값을 지정한다.\n",
    "    def __init__(self, gridSize):\n",
    "        # 그리드 개수\n",
    "        self.gridSize = gridSize\n",
    "        \n",
    "        # 입력 데이터 개수(그리드 개수, 그리드 개수)\n",
    "        self.state_size = self.gridSize * self.gridSize\n",
    "        \n",
    "        # 결과를 담을 행렬\n",
    "        self.stae = np.empty(3, dtype=np.uint8)\n",
    "    \n",
    "    # 관찰 결과를 리턴\n",
    "    def observe(self):\n",
    "        # 현재 게임 화면 상태를 받아온다.\n",
    "        canvas = self.drawState()\n",
    "        # 1차원 행렬로 변환한다.\n",
    "        canvas = np.reshape(canvas, (-1, self.state_size))\n",
    "        return canvas\n",
    "    \n",
    "    # 게임 현재 상태를 구한다.\n",
    "    def drawState(self):\n",
    "        # 입력데이터의 수만큼의 행렬을 구한다.\n",
    "        canvas = np.zeros((self.gridSize, self.gridSize))\n",
    "        \n",
    "        # 과일을 셋팅한다.\n",
    "        canvas[self.state[0] - 1, self.state[1] - 1] = 1\n",
    "        \n",
    "        # 바구니를 셋팅한다.\n",
    "        canvas[self.gridSize - 1, self.state[2] - 1 - 1] = 1\n",
    "        canvas[self.gridSize - 1, self.state[2] - 1] = 1\n",
    "        canvas[self.gridSize - 1, self.state[2] - 1 + 1] = 1        \n",
    "        \n",
    "        return canvas\n",
    "\n",
    "    # 게임을 초기상태로 리셋한다.\n",
    "    def reset(self):\n",
    "        # 초기 과일 위치 초기화\n",
    "        initialFruitColumn = random.randrange(1, self.gridSize + 1)\n",
    "        \n",
    "        # 초기 바구니 위치 초기화\n",
    "        initialBucketPosition = random.randrange(2, self.gridSize + 1 - 1)\n",
    "        \n",
    "        # 현재상태를 담아준다.\n",
    "        self.state = np.array([1, initialFruitColumn, initialBucketPosition])\n",
    "        return self.getState()\n",
    "    \n",
    "    #  현재 상태를 불러온다.\n",
    "    def getState(self):\n",
    "        stateinfo = self.state\n",
    "        \n",
    "        # 과일의 가로, 세로 위치\n",
    "        fruit_row = stateinfo[0]\n",
    "        fruit_col = stateinfo[1]\n",
    "        \n",
    "        # 바구니의 가로 위치\n",
    "        basket = stateinfo[2]\n",
    "        \n",
    "        return fruit_row, fruit_col, basket\n",
    "    \n",
    "    # 에이전트가 취한 행동에 대한 보상을 줍니다.\n",
    "    def getTeward(self):\n",
    "        # 각각의 위치값을 추출한다.\n",
    "        fruitRow, fruitCol, basket = self.getState()\n",
    "    \n",
    "        # 과일이 바닥에 닿았을 경우\n",
    "        if fruitRow == self.gridSize - 1:\n",
    "            # 바구니가 과일을 받았다면 보상을 1로 준다.\n",
    "            if abs(fruitCol - basket) <=1:\n",
    "                return 1\n",
    "            # 과일을 받지 못했다면 보상을 -1로 준다.\n",
    "            else:\n",
    "                return -1\n",
    "            \n",
    "        # 과일이 바닥에 닿지 않았을 경우\n",
    "        else:\n",
    "            # 아직 바닥에 닿지 않았으므로 0으로 보상을 준다.\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    # 게임이 끝났는지 확인한다. (1판 종료)\n",
    "    def isGameOver(self):\n",
    "        # 과일이 바닥에 닿았는지 검사\n",
    "        if self.state[0] == self.gridSize - 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # action(좌, 제자리, 우)에 따라 바구니와 과일의 위치를 수정한다.\n",
    "    def updateState(self, action):\n",
    "        move = 0\n",
    "        if action == 0:\n",
    "            move = -1\n",
    "        elif action == 1:\n",
    "            move = 0\n",
    "        elif action == 2:\n",
    "            move = 1\n",
    "    \n",
    "        # 현재 과일과 바구니 위치를 가져온다.\n",
    "        fruitRow, fruitCol, basket = self.getState()\n",
    "        \n",
    "        # 바구니의 위치를 업데이트 한다. (min, max는 grid 밖으로 벗어나는 것을 방지)\n",
    "        newBasket = min(max(2, basket + move), self.gridSize - 1)\n",
    "        \n",
    "        # 과일을 아래로 한칸씩 내린다.\n",
    "        fruitRow = fruitRow + 1\n",
    "        \n",
    "        # 현재 상태로 다시 셋팅한다.\n",
    "        self.state = np.array([fruitRow, fruitCol, newBasket])\n",
    "        \n",
    "    # 행동을 한다.\n",
    "    def act(self, action):\n",
    "        # action에 따라 현재 상태를 갱신한다.\n",
    "        self.updateState(action)\n",
    "        \n",
    "        # 갱신된 상태를 보고 보상을 결정한다.\n",
    "        reward = self.getReward()\n",
    "        \n",
    "        # 현재 한판이 끝났는지 확인한다.\n",
    "        gameOver = self.isGameOver()\n",
    "        \n",
    "        return self.observe(), reward, gameOver, self.getState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Memory를 class로 정의한다.\n",
    "class ReplayMemory():\n",
    "    def __init__(self, gridSize, maxMemory, discount):\n",
    "        # 초기값 설정\n",
    "        # 사용할 최대 메모리량\n",
    "        self.maxMemory = maxMemory\n",
    "        \n",
    "        # 게임환경의 가로나 세로 칸의 개수\n",
    "        self.gridSize = gridSize\n",
    "        \n",
    "        # 가로 * 세로 칸의 수\n",
    "        self.state_size = self.gridSize * self.gridSize\n",
    "        \n",
    "        # 규제강도\n",
    "        self.discount = discount\n",
    "        \n",
    "        # 게임 데이터를 담을 행렬 생성\n",
    "        canvas = np.zeros((self.gridSize * self.gridSize))\n",
    "        canvas = np.reshape(canvas, (-1, self.state_size))\n",
    "        \n",
    "        # 현재의 게임 상태를 담을 행렬\n",
    "        self.inputState = np.empty((self.maxMemory,100), dtype=np.float32)\n",
    "        # 에이전트의 행동 데이터를 담을 행렬\n",
    "        self.actions = np.zeros(self.maxMemory, dtype=np.uint8)\n",
    "        # 에이전트가 행동을 취한 다음 게임 상태를 담을 행렬\n",
    "        self.nextState = np.empty((self.maxMemory, 100), dtype=np.float32)\n",
    "        # 게임 오버여부\n",
    "        self.gameOver = np.empty(self.maxMemory, dtype=np.bool)\n",
    "        # 보상\n",
    "        self.rewards = np.empty(self.maxMemory, dtype=np.int8)\n",
    "        \n",
    "        # 몇번 플레이를 했는지\n",
    "        self.count = 0\n",
    "        # 현재 보상의 결과\n",
    "        self.current = 0\n",
    "    \n",
    "    # 경험을 저장한다.\n",
    "    def remember(self, currentState, action, reward, nextState, gameOver) :\n",
    "        self.actions[self.current] = action\n",
    "        self.rewards[self.current] = reward\n",
    "        self.inputState[self.current, ...] = currentState\n",
    "        self.nextState[self.current, ...] = nextState\n",
    "        self.gameOver[self.current] = gameOver\n",
    "        self.count = max(self.count, self.current + 1)\n",
    "        self.current = (self.current + 1) % self.maxMemory\n",
    "    \n",
    "    # 입력과 학습을 준비한다.\n",
    "    def getBatch(self, y_pred, batch_size, num_actions, state_size, sess, X):\n",
    "        # 위에서 설정한 batch_size와 최대 메모리와 비교하여\n",
    "        # 더 작은 값을 구한다.\n",
    "        memoryLength = self.count\n",
    "        chosenBatchSize = min(batchsize, memoryLength)\n",
    "        \n",
    "        # 입력과 결과 데이터를 담을 행렬을 생성한다.\n",
    "        inputs = np.zeros((chosenBatchSize, state_size))\n",
    "        targets = np.zeros((chosenBatchSize, num_actions))\n",
    "        \n",
    "        # 배치안에서 값을 추출해 담는다.\n",
    "        for i in range(chosenBatchSize):\n",
    "            # 배치에 포함될 기억을 랜덤으로 선택한다.\n",
    "            randomindex = random.randrange(0, memoryLength)\n",
    "            # 현재 상태와 0값을 불러온다.\n",
    "            current_inputState = np.reshape(self.inputState[randomindex], (1,100))\n",
    "            target = sess.run(y_pred, feed_dict={X:current_inputState})\n",
    "            \n",
    "            # 바로 다음 상태를 불러오고 다음 상태에서 얻을 수 있는 가장 큰 Q값을 계산\n",
    "            current_nextState = np.reshape(self.nextState[randomindex], (1,100))\n",
    "            nextStateQ = sess.run(y_pred, feed_dict = {X:current_nextState})\n",
    "            nextStateMaxQ = np.amax(nextStateQ)\n",
    "            \n",
    "            # 만약 게임오버 (과일이 바닥에 닿은 경우)라면 reward로 Q값을 업데이트\n",
    "            if self.gameOver[randomindex] == True:\n",
    "                target[self.actions[randomindex]] = self.rewards[randomindex]\n",
    "            \n",
    "            # 게임오버가 아니라면 타겟 Q값을 계산한다.\n",
    "            else:\n",
    "                target[self.actions[randomindex]] = self.rewards[randomindex] + self.discount + nextStateMaxQ\n",
    "                \n",
    "            # 인풋과 타겟 데이터에 값을 저장한다.\n",
    "            inputs[i] = current_inputState\n",
    "            targets[i] = target\n",
    "            \n",
    "        # 결과를 반환한다.\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로를 통해 학습 가동시 자동으로 호출되는 함수\n",
    "def main(a1):\n",
    "    print('학습을 시작합니다.')\n",
    "    \n",
    "    # 게임 플레이 환경을 선언한다.\n",
    "    env = CatchEnviroment(gridSize)\n",
    "    \n",
    "    # ReplayMemory를 선언한다.\n",
    "    memory = ReplayMemory(gridSize, maxMemory, discount)\n",
    "    \n",
    "    # 학습된 파라미터를 저장하기 위한 Saver를 선언한다.\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # 과일을 받은 수\n",
    "    winCount = 0\n",
    "    \n",
    "    # with문을 통해 작업이 완료되면 텐서플로가 자동 종료되게 한다.\n",
    "    with tf.Session() as sess:\n",
    "        # 변수들의 초기값을 할당한다.\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # 학습회수만큼 반복한다.\n",
    "        for i in range(num_epochs + 1):\n",
    "            # 환경을 초기화 한다.\n",
    "            err = 0\n",
    "            env.reset()\n",
    "            \n",
    "            isGameOver = False\n",
    "            \n",
    "            # 최초 상태를 가져온다.\n",
    "            currentState = env.observe()\n",
    "            \n",
    "            # 과일이 바닥에 닿을때까지 반복한다.\n",
    "            while isGameOver != True:\n",
    "                # Q값을 초기화한다.\n",
    "                action = -9999\n",
    "                \n",
    "                global epsilon\n",
    "                \n",
    "                # 만약 0 ~ 1 사이의 랜덤값이 엡실론 값보다 작거나 같으면\n",
    "                if randf(0,1) <= epsilon:\n",
    "                    # 랜덤한 행동으로 지정한다.\n",
    "                    action = random.randrange(0, num_actions)\n",
    "                    \n",
    "                # 그렇지 않으면\n",
    "                else: \n",
    "                    # 각 행동에 대한 q값을 구한다.\n",
    "                    q = sess.run(y_pred, feed_dict = {x:currentState})\n",
    "                    # q가 가장 큰 행동을 담아준다.\n",
    "                    action = q.argmax()\n",
    "                    \n",
    "                # epsilon 값에 0.999 값을 곱해서 epsilon값을 조정한다.\n",
    "                if epsilon > epsilonMinimumValue:\n",
    "                    epsilon = epsilon * 0.999\n",
    "                    \n",
    "                # 에이전트가 구한 행동을 통해 보상과 다음 상태를 가져온다.\n",
    "                nextState, reward, gameOver, stateinfo = env.act(action)\n",
    "                \n",
    "                # 만약 과일을 받아냈다면 점수를 1증가한다.\n",
    "                if reward == 1:\n",
    "                    winCount = winCount + 1\n",
    "                    \n",
    "                # 에이전트가 행동한 결과를 replayMemory에 저장한다.\n",
    "                memory.remember(currentState, action, reward, nextState, gameOver)\n",
    "                \n",
    "                # 다음 이동을 위해 현재 상태를 재설정한다.\n",
    "                currentState = nextState\n",
    "                \n",
    "                # 게임 오버 여부를 담는다.\n",
    "                isGameOver = gameOver\n",
    "                \n",
    "                # ReplayMemory로 부터 학습에 사용할 Batch 데이터를 불러온다.\n",
    "                inputs, targets = memory.getBatch(y_pred, batch_size, num_actions, state_size, sess, x)\n",
    "                \n",
    "                # 최적화(가중치 수정)을 수행하고 손실함수를 반환받는다.\n",
    "                _, loss_print = sess.run([optimizer, loss], feed_dict = {x:inputs, y:targets})\n",
    "                \n",
    "                # 손실율을 누적한다.\n",
    "                err = err + loss_print\n",
    "                \n",
    "            a100 = float(winCount) / float(i + 1) *100\n",
    "            print(f'반복 : {i}, 에러 : {err},승리 : {winCount},승리비율 : {a100}')\n",
    "        \n",
    "        print('학습완료')\n",
    "        save_path = saver.save(sess,'model.ckpt')\n",
    "        print('저장되었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습을 시작합니다.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CatchEnviroment' object has no attribute 'getReward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-cf34dbe789c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m     38\u001b[0m   \u001b[0mmain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m   \u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_parse_flags_tolerate_undef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\absl\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv, flags_parser)\u001b[0m\n\u001b[0;32m    301\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m       \u001b[0m_run_main\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m       \u001b[0musage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\absl\\app.py\u001b[0m in \u001b[0;36m_run_main\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-d0d028fc7ccf>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(a1)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[1;31m# 에이전트가 구한 행동을 통해 보상과 다음 상태를 가져온다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                 \u001b[0mnextState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgameOver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstateinfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[1;31m# 만약 과일을 받아냈다면 점수를 1증가한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-d0a6be01502b>\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# 갱신된 상태를 보고 보상을 결정한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetReward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;31m# 현재 한판이 끝났는지 확인한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CatchEnviroment' object has no attribute 'getReward'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
